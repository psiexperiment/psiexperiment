import logging
log = logging.getLogger(__name__)

import threading

from atom.api import Unicode, Typed, Property, Bool
from enaml.application import deferred_call
from enaml.layout.api import InsertItem
from enaml.widgets.api import DockItem
from enaml.workbench.api import Extension, PluginManifest
from enaml.workbench.core.api import Command

import tables as tb
import pandas as pd
import numpy as np

from ..plugin import Sink
from psi import get_config
from psi.core.utils import find_extension
from psi.controller.api import ExperimentAction

from .data_source import DataChannel, DataTable

PLUGIN_ID = 'psi.data.hdf_store'

MONKEYPATCH_COMPLETE = False


def monkeypatch_pytables():
    log.debug('Monkeypatching pytables')
    global MONKEYPATCH_COMPLETE
    if MONKEYPATCH_COMPLETE:
        return

    monkeypatch = [
        tb.Table.append,
        tb.Table.read,
        tb.Table.read_where,
        tb.EArray.append,
        tb.Array.__getitem__,
    ]

    def secure_lock(f, lock):
        def wrapper(*args, **kwargs):
            with lock:
                return f(*args, **kwargs)
        return wrapper

    lock = threading.Lock()
    for im in monkeypatch:
        wrapped_im = secure_lock(im, lock)
        setattr(im.im_class, im.im_func.func_name, wrapped_im)

    MONKEYPATCH_COMPLETE = True


class HDFStore(Sink):
    '''
    Simple class for storing acquired trial data in a HDF5 file. No analysis or
    further processing is done.
    '''
    file_handle = Property()
    node = Typed(tb.Group)
    initialized = Bool(False)

    trial_log = Typed(object)
    trial_log_dtype = Typed(np.dtype)
    event_log = Typed(object)
    event_log_dtype = Typed(np.dtype)

    _channels = Typed(dict)

    def _get_file_handle(self):
        return self.node._v_file

    def _prepare_trial_log(self, context_info):
        '''
        Create a table to hold the event log.
        '''
        dtype = [(str(name), item['dtype']) for name, item \
                 in context_info.items()]
        self.trial_log_dtype = np.dtype(dtype)
        node = self.file_handle.create_table(self.node, 'trial_log',
                                             self.trial_log_dtype)
        self.trial_log = DataTable(data=node)

    def _prepare_event_log(self):
        '''
        Create a table to hold the event log.
        '''
        dtype = [('timestamp', np.dtype('float32')), 
                 ('event', np.dtype('S512'))]
        self.event_log_dtype = np.dtype(dtype)
        node = self.file_handle.create_table(self.node, 'event_log',
                                             self.event_log_dtype)
        self.event_log = DataTable(data=node)

    def _prepare_continuous_input(self, input):
        atom = tb.Atom.from_dtype(input.channel.dtype)
        expected_rows = int(input.channel.fs * 60 * 60)
        filters = tb.Filters(**get_config('H5_COMPRESSION'))
        node = self.file_handle.create_earray(self.node, input.name, atom,
                                              (0,), filters=filters,
                                              expectedrows=expected_rows)
        for name, value in input.__getstate__().items():
            node._v_attrs[name] = value
        for name, value in input.channel.__getstate__().items():
            node._v_attrs['channel_' + name] = value
        for name, value in input.engine.__getstate__().items():
            node._v_attrs['engine_' + name] = value
        return DataChannel(data=node, fs=input.fs) 

    def _prepare_inputs(self, inputs):
        channels = {}
        for input in inputs:
            log.debug('Preparing file for input {}'.format(input.name))
            prep_function_name = '_prepare_{}_input'.format(input.mode)
            if not hasattr(self, prep_function_name):
                m = 'No method for preparing datasource {} of type {}'
                log.debug(m.format(input.name, input.mode))
            prep_function = getattr(self, prep_function_name)
            channels[input.name] = prep_function(input)

        self._channels = channels

    def prepare(self, plugin):
        self._prepare_event_log()
        self._prepare_trial_log(plugin.context_info)
        self._prepare_inputs(plugin.inputs.values())
        self._channels['trial_log'] = self.trial_log
        self._channels['event_log'] = self.event_log
        self.initialized = True

    def finalize(self):
        log.debug('Flushing all data to disk')
        self.file_handle.flush()

    def stop(self):
        log.debug('Closing HDF5 file')
        self.file_handle.close()

    def process_trial(self, results):
        # This is the simplest one-liner to convert the dictionary to the
        # format required for appending.
        row = pd.DataFrame([results]).to_records().astype(self.trial_log_dtype)
        self.trial_log.append(row)

    def process_event(self, event, timestamp):
        data = {'event': event, 'timestamp': timestamp}
        row = pd.DataFrame([data]).to_records().astype(self.event_log_dtype)
        self.event_log.append(row)

    def process_ai(self, name, data):
        if self._channels[name] is not None:
            self._channels[name].append(data)

    def get_source(self, source_name):
        try:
            return self._channels[source_name]
        except KeyError:
            # TODO: Once we port to Python 3, add exception chaining.
            raise AttributeError

    def set_current_time(self, name, timestamp):
        self._channels[name].set_current_time(timestamp)


def set_node(event):
    extension = find_extension(event.workbench, PLUGIN_ID, 'sink', HDFStore)
    if extension.initialized:
        raise ValueError('Cannot change node once data has initialized')
    extension.node = event.parameters['node']


enamldef HDFStoreManifest(PluginManifest): manifest:

    id = PLUGIN_ID

    Extension:
        id = 'sink'
        point = 'psi.data.sink'
        HDFStore:
            pass

    Extension:
        id = 'commands'
        point = 'enaml.workbench.core.commands'
        Command:
            id = PLUGIN_ID + '.set_node'
            handler = set_node

    Extension:
        id = 'actions'
        point = 'psi.controller.actions'
        ExperimentAction:
            event = 'experiment_initialize'
            command = 'psi.data.prepare'


monkeypatch_pytables()
